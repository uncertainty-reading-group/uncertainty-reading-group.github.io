---
layout: post
title: Pitfalls of Epistemic Uncertainty Quantification through Loss Minimisation
subtitle: Viktor Bengs - NeurIPS 2022
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/cover-bengs.png
share-img: /assets/img/path.jpg
tags: [Epistemic Uncertainty]
comments: false
---

[**Pitfalls of Epistemic Uncertainty Quantification through Loss Minimisation**](https://openreview.net/pdf?id=epjxT_ARZW5)

by Viktor Bengs, Eyke Hüllermeier, and Willem Waegeman
Published at the Conference on Neural Information Processing Systems (NeurIPS), 2022

**Abstract**

Uncertainty quantification has received increasing attention in machine learning in the recent past. In particular, a distinction between aleatoric and epistemic uncertainty has been found useful in this regard. The latter refers to the learner's (lack of) knowledge and appears to be especially difficult to measure and quantify. In this paper, we analyse a recent proposal based on the idea of a second-order learner, which yields predictions in the form of distributions over probability distributions. While standard (first-order) learners can be trained to predict accurate probabilities, namely by minimising suitable loss functions on sample data, we show that loss minimisation does not work for second-order predictors: The loss functions proposed for inducing such predictors do not incentivise the learner to represent its epistemic uncertainty in a faithful way.

**Presenter**

Viktor Bengs is currently a Postdoctoral Researcher at LMU Munich at the [chair of Artificial Intelligence and Machine Learning](https://www.kiml.ifi.lmu.de/index.html) of Prof. Dr. Eyke Hüllermeier.

**Links**

[Paper](https://openreview.net/pdf?id=epjxT_ARZW5)
