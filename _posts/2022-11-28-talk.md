---
layout: post
title: NOMU - Neural Optimization-based Model Uncertainty
subtitle: Jakob Heiss - ICML 2022
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/cover-pinto.png
share-img: /assets/img/path.jpg
tags: [Uncertainty]
comments: false
---

This page is about the presentation of the paper

[**NOMU: Neural Optimization-based Model Uncertainty**](https://arxiv.org/pdf/2102.13640.pdf)

by Jakob Heiss, Jakob Weissteiner, Hanna Wutte, Sven Seuken, Josef Teichmann
Published at the International Conference on MachineLearning (ICML), 2022

**Abstract**

We study methods for estimating model uncer-tainty for neural networks (NNs) in regression.To isolate the effect of model uncertainty, we fo-cus on a noiseless setting with scarce training data.We introduce five important desiderata regardingmodel uncertainty that any method should satisfy.However, we find that established benchmarks of-ten fail to reliably capture some of these desider-ata, even those that are required by Bayesian the-ory. To address this, we introduce a new approachfor capturing model uncertainty for NNs, whichwe  callNeural  Optimization-based  Model  Un-certainty (NOMU). The main idea of NOMU isto  design  a  network  architecture  consisting  oftwo connected sub-NNs, one for model predic-tion and one for model uncertainty, and to train itusing a carefully-designed loss function. Impor-tantly, our design enforces that NOMU satisfiesour five desiderata. Due to its modular architec-ture, NOMU can provide model uncertainty forany given (previously trained) NN if given accessto its training data. We evaluate NOMU in variousregressions tasks and noiseless Bayesian optimiza-tion (BO) with costly evaluations. In regression,NOMU performs at least as well as state-of-the-art methods. In BO, NOMU even outperforms allconsidered benchmarks.

**Presenter**


**Links**

[Paper](https://arxiv.org/pdf/2102.13640.pdf)
